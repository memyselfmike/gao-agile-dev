# Todo App Baseline Benchmark Configuration

benchmark:
  name: "todo-app-baseline"
  description: "Full-stack todo application with authentication - baseline benchmark"
  version: "1.0.0"

  # The prompt that kicks off autonomous development
  initial_prompt: |
    Build a todo application with the following features:

    **Core Features:**
    - User authentication (register, login, logout)
    - Create, read, update, delete todos
    - Organize todos with categories/tags
    - Set due dates and priority levels
    - Responsive UI with accessibility support

    **Technical Requirements:**
    - Use the provided Next.js boilerplate as starting point
    - Add API routes for backend
    - Use PostgreSQL database with Prisma ORM
    - Implement JWT-based authentication
    - Write comprehensive tests (>80% coverage)
    - Ensure zero TypeScript errors
    - Include Docker deployment configuration

    **Quality Standards:**
    - Follow WCAG 2.1 AA accessibility guidelines
    - RESTful API design
    - Comprehensive error handling
    - Security best practices (input validation, XSS prevention, etc.)

  # Technology stack
  tech_stack:
    frontend: "nextjs"          # Next.js 15 with App Router
    backend: "nextjs-api-routes" # Next.js API routes (or "fastapi" for separate backend)
    database: "postgresql"      # PostgreSQL
    orm: "prisma"              # Prisma ORM
    auth: "jwt"                # JWT authentication
    test_framework: "jest"     # Jest + React Testing Library
    deployment: "docker"       # Docker + docker-compose

  # Boilerplate configuration
  boilerplate:
    repo_url: "https://github.com/webventurer/simple-nextjs-starter"
    branch: "main"

    # Template variable substitutions
    substitutions:
      PROJECT_NAME: "todo-app"
      AUTHOR: "GAO-Dev Team"
      DESCRIPTION: "Full-stack todo application with authentication"
      repository_url: "https://github.com/memyselfmike/todo-app-benchmark"
      username: "memyselfmike"

  # Success criteria that must be met
  success_criteria:
    - name: "All tests passing"
      type: "test_results"
      threshold: 100  # 100% of tests must pass
      required: true

    - name: "Code coverage"
      type: "coverage"
      threshold: 80  # Minimum 80% coverage
      required: true

    - name: "Zero type errors"
      type: "type_check"
      threshold: 0  # Zero TypeScript errors allowed
      required: true

    - name: "Zero linting errors"
      type: "lint"
      threshold: 0
      required: true

    - name: "Authentication working"
      type: "functional"
      validator: "test_auth_flow"  # Custom test
      required: true

    - name: "CRUD operations working"
      type: "functional"
      validator: "test_todo_crud"
      required: true

    - name: "Categories working"
      type: "functional"
      validator: "test_categories"
      required: true

    - name: "API documented"
      type: "documentation"
      required: true

    - name: "Docker deployment working"
      type: "deployment"
      validator: "test_docker_compose"
      required: true

  # Metrics to collect
  metrics_enabled:
    performance: true   # Time, tokens, API calls
    autonomy: true      # Interventions, success rates
    quality: true       # Coverage, errors, vulnerabilities
    workflow: true      # Story cycle time, phases

  # Execution settings
  timeout: 3600  # 1 hour maximum
  max_retries: 3  # Retry failed operations up to 3 times

  # Phases to execute (optional - default is all)
  phases:
    - analysis      # Mary: Business analysis
    - planning      # John: PRD creation
    - solutioning   # Winston: Architecture, Sally: UX design
    - implementation # Bob: Stories, Amelia: Development, Murat: Testing

  # Agent configuration (optional overrides)
  agents:
    mary:
      enabled: true
    john:
      enabled: true
    winston:
      enabled: true
    sally:
      enabled: true
    bob:
      enabled: true
    amelia:
      enabled: true
    murat:
      enabled: true

  # Output configuration
  output:
    save_artifacts: true     # Save all generated files
    save_metrics: true       # Persist metrics to database
    generate_report: true    # Generate HTML report
    export_metrics: "json"   # Export format (json, csv, both)

---

# Notes:
# - This is the baseline benchmark - simplest successful run establishes baseline metrics
# - Use this config with: gao-dev sandbox run benchmarks/todo-baseline.yaml
# - Results will be in sandbox/projects/todo-app-<timestamp>/
# - Metrics will be stored and can be viewed with: gao-dev sandbox report <run-id>
